% Encoding: UTF-8

@Electronic{Ng-Coursera-2016,
  author     = {Ng},
  year       = {2016},
  title      = {Stanford Course on Machine Learning},
  url        = {https://www.coursera.org/learn/machine-learning},
  bdsk-url-1 = {https://www.coursera.org/learn/machine-learning},
}

@Article{Miikkulainen2017,
  author     = {Miikkulainen, Risto and Liang, Jason Zhi and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
  title      = {Evolving Deep Neural Networks},
  journal    = {CoRR},
  year       = {2017},
  volume     = {abs/1703.00548},
  bdsk-url-1 = {http://arxiv.org/abs/1703.00548},
  bibsource  = {dblp computer science bibliography, http://dblp.org},
  biburl     = {http://dblp.org/rec/bib/journals/corr/MiikkulainenLMR17},
  timestamp  = {Wed, 07 Jun 2017 01:00:00 +0200},
  url        = {http://arxiv.org/abs/1703.00548},
}

@Article{Schmidhuber2015,
  author     = {Schmidhuber, J.},
  title      = {Deep Learning in Neural Networks: An Overview},
  journal    = {Neural Networks},
  year       = {2015},
  volume     = {61},
  pages      = {85-117},
  note       = {Published online 2014; based on TR arXiv:1404.7828 [cs.NE]},
  bdsk-url-1 = {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
  doi        = {10.1016/j.neunet.2014.09.003},
}

@Article{DBLP:journals/corr/VeniatD17,
  author    = {Tom Veniat and Ludovic Denoyer},
  title     = {Learning Time-Efficient Deep Architectures with Budgeted Super Networks},
  journal   = {CoRR},
  year      = {2017},
  volume    = {abs/1706.00046},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/VeniatD17},
  timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
  url       = {http://arxiv.org/abs/1706.00046},
}

@Article{DBLP:journals/corr/MocanuMSNGL17,
  author    = {Decebal Constantin Mocanu and Elena Mocanu and Peter Stone and Phuong H. Nguyen and Madeleine Gibescu and Antonio Liotta},
  title     = {Evolutionary Training of Sparse Artificial Neural Networks: {A} Network Science Perspective},
  journal   = {CoRR},
  year      = {2017},
  volume    = {abs/1707.04780},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MocanuMSNGL17},
  timestamp = {Sat, 05 Aug 2017 14:56:16 +0200},
  url       = {http://arxiv.org/abs/1707.04780},
}

@Article{2017arXiv170909161D,
  author        = {{Dufourq}, E. and {Bassett}, B.~A.},
  title         = {{EDEN: Evolutionary Deep Networks for Efficient Machine Learning}},
  journal       = {ArXiv e-prints},
  year          = {2017},
  month         = sep,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170909161D},
  archiveprefix = {arXiv},
  eprint        = {1709.09161},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
  primaryclass  = {stat.ML},
}

@InProceedings{Gomez:2005:CRN:1068009.1068092,
  author    = {Gomez, Faustino J. and Schmidhuber, J\"{u}rgen},
  title     = {Co-evolving Recurrent Neurons Learn Deep Memory POMDPs},
  booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
  year      = {2005},
  series    = {GECCO '05},
  pages     = {491--498},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1068092},
  doi       = {10.1145/1068009.1068092},
  isbn      = {1-59593-010-8},
  keywords  = {POMDP, coevolution, recurrent neural networks},
  location  = {Washington DC, USA},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/1068009.1068092},
}

@Article{gomez2008accelerated,
  author  = {Gomez, Faustino and Schmidhuber, J{\"u}rgen and Miikkulainen, Risto},
  title   = {Accelerated neural evolution through cooperatively coevolved synapses},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {May},
  pages   = {937--965},
}

@Article{doi:10.1162/106365602320169811,
  author   = {Kenneth O. Stanley and Risto Miikkulainen},
  title    = {Evolving Neural Networks through Augmenting Topologies},
  journal  = {Evolutionary Computation},
  year     = {2002},
  volume   = {10},
  number   = {2},
  pages    = {99-127},
  abstract = { An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution. },
  doi      = {10.1162/106365602320169811},
  eprint   = {https://doi.org/10.1162/106365602320169811},
  url      = { 
        https://doi.org/10.1162/106365602320169811
    
},
}

@Conference{Papavasileiou20171431,
  author        = {Papavasileiou, E. and Jansen, B.},
  title         = {An Investigation of topological choices in Fs-NEAT and fd-neat on XOR-based problems of increased complexity},
  year          = {2017},
  pages         = {1431-1434},
  note          = {cited By 0},
  document_type = {Conference Paper},
  doi           = {10.1145/3067695.3082497},
  journal       = {GECCO 2017 - Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026862559&doi=10.1145%2f3067695.3082497&partnerID=40&md5=7c9bb7c91068b93bc8315f500d49e597},
}

@Conference{Wen2017,
  author        = {Wen, R. and Guo, Z. and Zhao, T. and Ma, X. and Wang, Q. and Wu, Z.},
  title         = {Neuroevolution of augmenting topologies based musculor-skeletal arm neurocontroller},
  year          = {2017},
  note          = {cited By 0},
  art_number    = {7969727},
  document_type = {Conference Paper},
  doi           = {10.1109/I2MTC.2017.7969727},
  journal       = {I2MTC 2017 - 2017 IEEE International Instrumentation and Measurement Technology Conference, Proceedings},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026767559&doi=10.1109%2fI2MTC.2017.7969727&partnerID=40&md5=45ddd0bf3a3e2982cbe22e5de4cac5aa},
}

@Article{Angeline199454,
  author        = {Angeline, P.J. and Saunders, G.M. and Pollack, J.B.},
  title         = {An Evolutionary Algorithm that Constructs Recurrent Neural Networks},
  journal       = {IEEE Transactions on Neural Networks},
  year          = {1994},
  volume        = {5},
  number        = {1},
  pages         = {54-65},
  note          = {cited By 549},
  document_type = {Article},
  doi           = {10.1109/72.265960},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028202641&doi=10.1109%2f72.265960&partnerID=40&md5=50ec25abf8a7b7c0dc8204067007255c},
}

@Conference{Whiteson20051225,
  author        = {Whiteson, S. and Stone, P. and Stanley, K.O. and Miikkulainen, R. and Kohl, N.},
  title         = {Automatic feature selection in neuroevolution},
  year          = {2005},
  pages         = {1225-1232},
  note          = {cited By 46},
  document_type = {Conference Paper},
  doi           = {10.1145/1068009.1068210},
  journal       = {GECCO 2005 - Genetic and Evolutionary Computation Conference},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-32444438066&doi=10.1145%2f1068009.1068210&partnerID=40&md5=bf49e8b65e4fe639405812a597dee578},
}

@InProceedings{Miikkulainen:2015:ENN:2739482.2756577,
  author    = {Miikkulainen, Risto},
  title     = {Evolving Neural Networks},
  booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  year      = {2015},
  series    = {GECCO Companion '15},
  pages     = {137--161},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {Neuroevolution, i.e. evolution of artificial neural networks, has recently emerged as a powerful technique for solving challenging reinforcement learning problems. Compared to traditional (e.g. value-function based) methods, neuroevolution is especially strong in domains where the state of the world is not fully known: The state can be disambiguated through recurrency, and novel situations handled through pattern matching. In this tutorial, I will review (1) neuroevolution methods that evolve fixed-topology networks, network topologies, and network construction processes, (2) ways of combining traditional neural network learning algorithms with evolutionary methods, and (3) applications of neuroevolution to control, robotics, artificial life, and games.},
  acmid     = {2756577},
  doi       = {10.1145/2739482.2756577},
  isbn      = {978-1-4503-3488-4},
  keywords  = {artificial life, control, game playing, neural networks, robotics},
  location  = {Madrid, Spain},
  numpages  = {25},
  url       = {http://doi.acm.org/10.1145/2739482.2756577},
}

@InBook{Kamioka2009,
  pages     = {22--31},
  title     = {NeuroEvolution Based on Reusable and Hierarchical Modular Representation},
  publisher = {Springer Berlin Heidelberg},
  year      = {2009},
  author    = {Kamioka, Takumi and Uchibe, Eiji and Doya, Kenji},
  editor    = {K{\"o}ppen, Mario and Kasabov, Nikola and Coghill, George},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-02490-0},
  abstract  = {The framework of neuroevolution (NE) provides a way of finding appropriate structures as well as connection weights of artificial neural networks. However, the conventional NE approach of directly coding each connection weight by a gene is severely limited in its scalability and evolvability. In this study, we propose a novel indirect coding approach in which a phenotypical network develops from the genes encoding multiple subnetwork modules. Each gene encodes a subnetwork consisting of the input, hidden, and output nodes and connections between them. A connection can be a real weight or a pointer to another subnetwork. The structure of the network evolves by inserting new connection weights or subnetworks, merging two subnetworks as a higher-level subnetwork, or changing the existing connections. We investigated the evolutionary process of the network structure using the task of double pole balancing. We confirmed that the proposed method by the modular developmental rule can produce a wide variety of network architectures and that evolution can trim them down to the most appropriate ones required by the task.},
  booktitle = {Advances in Neuro-Information Processing: 15th International Conference, ICONIP 2008, Auckland, New Zealand, November 25-28, 2008, Revised Selected Papers, Part I},
  doi       = {10.1007/978-3-642-02490-0_3},
  url       = {https://doi.org/10.1007/978-3-642-02490-0_3},
}

@Article{DBLP:journals/corr/abs-1004-3557,
  author    = {Eva Voln{\'{a}}},
  title     = {Neuroevolutionary optimization},
  journal   = {CoRR},
  year      = {2010},
  volume    = {abs/1004.3557},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1004-3557},
  timestamp = {Wed, 07 Jun 2017 14:42:54 +0200},
  url       = {http://arxiv.org/abs/1004.3557},
}

@InProceedings{Linhardt:2009:NIN:1570256.1570282,
  author    = {Linhardt, Matthias J. and Butz, Martin V.},
  title     = {NEAT in Increasingly Non-linear Control Situations},
  booktitle = {Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers},
  year      = {2009},
  series    = {GECCO '09},
  pages     = {2091--2096},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {1570282},
  doi       = {10.1145/1570256.1570282},
  isbn      = {978-1-60558-505-5},
  keywords  = {adaptive control, dynamic control, neat, neuroevolution},
  location  = {Montreal, Qu\&\#233;bec, Canada},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/1570256.1570282},
}

@PhdThesis{kohli2017evolving,
  author = {Kohli, Maitrei},
  title  = {Evolving Neural Networks Using Behavioural Genetic Principles},
  school = {Department of Computer Science \& Information Systems Birkbeck, University of London United Kingdom},
  year   = {2017},
}

@Article{784219,
  author   = {Xin Yao},
  title    = {Evolving artificial neural networks},
  journal  = {Proceedings of the IEEE},
  year     = {1999},
  volume   = {87},
  number   = {9},
  pages    = {1423-1447},
  month    = {Sep},
  issn     = {0018-9219},
  abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
  doi      = {10.1109/5.784219},
  keywords = {genetic algorithms;learning (artificial intelligence);neural nets;search problems;technological forecasting;connection weights;evolutionary algorithms;intelligent systems;learning;neural networks;search operators;Adaptive systems;Algorithm design and analysis;Artificial intelligence;Artificial neural networks;Competitive intelligence;Computer networks;Evolutionary computation;Intelligent networks;Intelligent systems;Transfer functions},
}

@InProceedings{Stanley:2002:ERL:2955491.2955578,
  author    = {Stanley, Kenneth O. and Miikkulainen, Risto},
  title     = {Efficient Reinforcement Learning Through Evolving Neural Network Topologies},
  booktitle = {Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation},
  year      = {2002},
  series    = {GECCO'02},
  pages     = {569--577},
  address   = {San Francisco, CA, USA},
  publisher = {Morgan Kaufmann Publishers Inc.},
  acmid     = {2955578},
  isbn      = {1-55860-878-8},
  location  = {New York City, New York},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2955491.2955578},
}

@Article{DBLP:journals/corr/RealMSSSLK17,
  author    = {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Quoc V. Le and Alex Kurakin},
  title     = {Large-Scale Evolution of Image Classifiers},
  journal   = {CoRR},
  year      = {2017},
  volume    = {abs/1703.01041},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RealMSSSLK17},
  timestamp = {Wed, 07 Jun 2017 14:41:10 +0200},
  url       = {http://arxiv.org/abs/1703.01041},
}

@InProceedings{icml2013_wan13,
  author    = {Li Wan and Matthew Zeiler and Sixin Zhang and Yann L. Cun and Rob Fergus},
  title     = {Regularization of Neural Networks using DropConnect},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
  year      = {2013},
  editor    = {Sanjoy Dasgupta and David Mcallester},
  volume    = {28},
  number    = {3},
  pages     = {1058-1066},
  month     = may,
  publisher = {JMLR Workshop and Conference Proceedings},
  abstract  = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
  url       = {http://jmlr.org/proceedings/papers/v28/wan13.pdf},
}

@Article{DBLP:journals/corr/Graham14a,
  author    = {Benjamin Graham},
  title     = {Fractional Max-Pooling},
  journal   = {CoRR},
  year      = {2014},
  volume    = {abs/1412.6071},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Graham14a},
  timestamp = {Wed, 07 Jun 2017 14:41:17 +0200},
  url       = {http://arxiv.org/abs/1412.6071},
}

@Article{DBLP:journals/corr/ClevertUH15,
  author    = {Djork{-}Arn{\'{e}} Clevert and Thomas Unterthiner and Sepp Hochreiter},
  title     = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  journal   = {CoRR},
  year      = {2015},
  volume    = {abs/1511.07289},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ClevertUH15},
  timestamp = {Wed, 07 Jun 2017 14:40:17 +0200},
  url       = {http://arxiv.org/abs/1511.07289},
}

@Electronic{Vanhoucke2017,
  author     = {Vanhoucke},
  year       = {2017},
  title      = {Google course on ML and Tensorflow},
  url        = {https://classroom.udacity.com/courses/ud730},
  bdsk-url-1 = {https://classroom.udacity.com/courses/ud730},
}

@Electronic{Stanley2017,
  author = {Stanley, K. O.},
  year   = {2017},
  title  = {Neuroevolution: A different kind of deep learning},
  url    = {https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning},
}

@Article{Radcliffe1993,
  author   = {Radcliffe, Nicholas J.},
  title    = {Genetic set recombination and its application to neural network topology optimisation},
  journal  = {Neural Computing {\&} Applications},
  year     = {1993},
  volume   = {1},
  number   = {1},
  pages    = {67--90},
  month    = {Mar},
  issn     = {1433-3058},
  abstract = {Forma analysis is applied to the task of optimising the connectivity of a feed- forward neural network with a single layer of hidden units. This problem is reformulated as a multiset optimisation problem, and techniques are developed to allow principled genetic search over fixed- and variable-size sets and multisets. These techniques require a further generalisation of the notion of gene, which is presented. The result is a non-redundant representation of the neural network topology optimisation problem, together with recombination operators which have carefully designed and well-understood properties. The techniques developed have relevance to the application of genetic algorithms to constrained optimisation problems.},
  day      = {01},
  doi      = {10.1007/BF01411376},
  url      = {https://doi.org/10.1007/BF01411376},
}

@Article{MP:MP3941,
  author    = {Tan, Maxine and Deklerck, Rudi and Jansen, Bart and Bister, Michel and Cornelis, Jan},
  title     = {A novel computer-aided lung nodule detection system for CT images},
  journal   = {Medical Physics},
  year      = {2011},
  volume    = {38},
  number    = {10},
  pages     = {5630--5645},
  issn      = {2473-4209},
  doi       = {10.1118/1.3633941},
  keywords  = {Computed tomography, Computed radiography, Segmentation, Blood-brain barrier, blood vessels, cancer, computerised tomography, feature extraction, genetic algorithms, image classification, image segmentation, lung, medical image processing, neural nets, support vector machines, tumours, medical image analysis, lung nodule detection, invariant image features, classifiers, FD-NEAT, Lungs, Medical imaging, Databases, Computed tomography, Radiologists, Image detection systems, Cancer, Topology, Networks, Computer aided diagnosis},
  publisher = {American Association of Physicists in Medicine},
  url       = {http://dx.doi.org/10.1118/1.3633941},
}

@InProceedings{miller1989designing,
  author    = {Miller, Geoffrey F and Todd, Peter M and Hegde, Shailesh U},
  title     = {Designing Neural Networks using Genetic Algorithms.},
  booktitle = {ICGA},
  year      = {1989},
  volume    = {89},
  pages     = {379--384},
}

@Comment{jabref-meta: databaseType:bibtex;}
