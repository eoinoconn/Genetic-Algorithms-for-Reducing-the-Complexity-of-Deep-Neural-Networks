\documentclass[]{monthly-report}
\def\modulename{ME Project} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% BEGINNING %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%
%%% Input your name, student number, 
%%% project and report details

\def\studentname{Eoin O'Connell}
\def\projecttitle{Genetic Algorithms for Reducing the Complexity of Deep Neural Networks }
\def\ucdstudentnumber{{13335561}}
\def\monthlyreportnumber{{1}}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%
%%% First Section


\section{Work Plan}

Between now and the next monthly report I will continue learning and investigating about deep neural networks. I am enrolled in a number of on-line courses. The machine learning course by Andrew Ng on Coursera~\cite{Ng-Coursera-2016} and the Google deep learning course on Udacity~\cite{Vanhoucke2017}. Both of these focus on the theory of neural networks but the Udacity course teaches how to use tensorflow also. I have coded in Python before but there will need be some learning on utilising it specifically for data science and machine learning over the coming month. I am also enrolled in UCDs machine learning course this semester which will focus on a broad number of ML techniques as well as neural networks. 

I hope to begin work on encoding neural network parameters for use in the GA before the next report.

Beyond the next report I hope to have a fully worked method of encoding the network parameters before returning from the Christmas break. I think this is reasonable as much of the time between now and then will be spent undertaking a crash course on deep neural networks and tackling one of the more complex problems of the project. From then on I hope to begin work on a basic method of neuroevolution whilst simultaneously reflecting on and deciding what novelties I could develop for my algorithm. The stretch from the mid-term break till May will be focussed on implementing that novelty into a fully functional genetic algorithm implementation. 

\section{Git \& Backups}

At present any code I am writing is strictly practise code for getting up to speed on the relevant tools and packages for the project. Once I begin development I will initialise a GitHub repository to manage the code. 

Simultaneously I utilise cloud backups across all the PCs I use to code and work on the project.

\section{Datasets and trained networks}

Some datasets with their trained models and state-of-the-art results.
\begin{itemize}

\item 96.53\% on CIFAR-10 with fractional max-pooling.~\cite{DBLP:journals/corr/Graham14a}

\item 75.72\% on CIFAR-100 using exponential linear units.~\cite{DBLP:journals/corr/ClevertUH15}

\item 99.79\% on NMIST using DropConnect.~\cite{icml2013_wan13}

\end{itemize}



\section{State-of-the-art}
Neuroevolution implements the automatic design of neural networks for classification or regression using genetic algorithms(GAs)\cite{Stanley2017}. GAs are a search space method of optimisation following the principals of natural selection. A population of chromosomes are generated, their fitness evaluated and the "fittest" chromosomes are combined to create better offspring for which the process once again repeats. This algorithm can be successfully applied to the automatic design of neural networks without the need to provide a predefined network topology. These networks are referred to as Topology and Weight Evolving Artificial Neural Networks (TWEANNs).   

The majority of state-of-the-art developments in the area of neuroevolution revolve around the improvement of previously developed procedures for automatically developing topologies for Deep Neural Networks (DNNs). The most prominent of these procedures is NeuroEvolution of Augmenting Topologies (NEAT). It proposed a combination of novelties to culminate in an efficient method for evolving topologies along with weights~\cite{Stanley:2002:ERL:2955491.2955578}. These novelties remain in more recent approaches such as in CoDeepNeat proposed by Miikkulainen et al~\cite{Miikkulainen2017}. Miikulainen proposes a method for evolving hyper-parameters(learning rate, training algorithm and data preprocessing) as well as topology and network weights. This new procedure, although computationally  intense to execute produces networks comparable to State-of-the-art for image classification on the Cifar-10 data set. NEAT develops a network by first starting minimally and then gradually complexifying the network over generations. This makes for significantly more efficient networks.

Many of NEATs novelties centred around its use of historical markers in each chromosome to aid in crossover and mutation. Previous GAs faced the Competing Conventions Problem. When chromosomes don't have identical encoding, crossover may cause some loss in functionality.~\cite{Radcliffe1993} Historical markers allow the algorithm to track identical structures (Extending our biological metaphor this would be similar to tracking species.) and avoid gene duplication or loss of features. The markers can also be used to protect innovation. The introduction of a new connection to a network, through mutation, may negatively impact the fitness of the chromosome but given time to train the connection may improve the fitness overall. Organising chromosomes into species we can apply a method known as explicit feature sharing so that chromosomes with innovations are protected by the species at large and don't have to compete with the global population. 

Other offshoot of NEAT include Feature selection and Feature De-selection NEAT (FS-NEAT~\cite{Whiteson20051225}, FD-NEAT~\cite{MP:MP3941}). These both focus on similar methods for automating the selection of features as well as network weights and topologies. These methods provide computational efficiency by removing potentially irrelevant or unhelpful features.

In order to reduce the complexity of produced neural networks, it will be important to examine fully connected versus sparsely connected network topologies. Research conducted by Mocanu et al.\cite{DBLP:journals/corr/MocanuMSNGL17} sets out a different procedure referred to as Sparse Evolutionary Training (SET). This is a specific method for training sparsely connected networks with much smaller computational intensity. The procedure was tested on two common network types, Restricted Boltzman machine and Multi-layer Perceptron, against 14 dataset benchmarks.  In each case SET quadratically reduced the complexity of the neural networks whilst maintaining or surpassing the accuracy of the state-of-the-art fully connected networks.

A successful fully connected model referred to as Evolutionary DEep Networks or EDEN has been tested against a number of NMIST offshoot datasets and equalled or in some cases surpassed the state-of-the-art~\cite{2017arXiv170909161D}. This approach is significantly different than previous approaches as it reduces the complexity of the genetic algorithm itself to run on a single powerful GPU. The algorithm will require a method of evaluating the fitness of the generated networks. EDEN utilises a tournament style fitness function where only a small portion of the population each generation is evaluated the best are used to generate offspring.  

To develop a method of neuroevolution we will need to encode the parameters of a DNN into chromosomes to apply the methods of natural selection (Speciation, mutation, crossover etc.). Both NEAT and CoDeepNEAT encoded the genes using a graph structure. Where they differed however was that in CoDeepNEAT each gene represented a layer as oppose to a single neuron like with NEAT. 


%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
\begingroup
\raggedright
\bibliography{report-biblio}{}
\endgroup
\bibliographystyle{IEEEtran}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% END %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{last_page}

 \end{document} 